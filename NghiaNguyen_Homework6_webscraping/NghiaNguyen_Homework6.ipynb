{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f24f71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "from parsel import Selector\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73947c03",
   "metadata": {},
   "source": [
    "# Nasa Satellite images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff6bced8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_image(url, folder_path, file_name):\n",
    "    \"\"\"\n",
    "    Download the image from the given URL and save it with the specified file name in the specified folder path.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name + '.jpg')\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# URL and folder path\n",
    "url = \"https://earthobservatory.nasa.gov/images\"\n",
    "folder_path = r\"C:\\Users\\darvi\\Desktop\\GitHub\\Python-Assignment1\"\n",
    "folder_path = folder_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Folder name\n",
    "folder_name = \"Nasa Satellite Images\"\n",
    "file_path = os.path.join(folder_path, folder_name)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)\n",
    "    \n",
    "opts = Options()\n",
    "driver = webdriver.Chrome(options=opts, executable_path=\"chromedriver\")\n",
    "\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "Number_of_click = 3 # Adjust the number of times you want to click the \"Explore More\" button\n",
    "while Number_of_click > 0:\n",
    "    Number_of_click -= 1\n",
    "    explore_more_button = driver.find_element(By.XPATH, '//*[@class=\"explore-more\"]')\n",
    "    explore_more_button.click()\n",
    "    sleep(1)  # Add a delay of 2 seconds to allow the content to load\n",
    "\n",
    "page_source = driver.page_source\n",
    "# driver.quit()\n",
    "\n",
    "soup = BeautifulSoup(page_source, \"lxml\")\n",
    "\n",
    "Nasa_Satellite_images = {}\n",
    "image_links = soup.find_all(\"div\", class_=\"thumbnail-image\")\n",
    "captions = soup.find_all(\"div\", class_=\"caption\")\n",
    "\n",
    "for link, caption in zip(image_links, captions):\n",
    "    \n",
    "    # Get the download link, description, and title\n",
    "    download_link = link.a.img[\"src\"]\n",
    "    description = caption.p.text\n",
    "    title = caption.h4.a.text\n",
    "    \n",
    "    # Download the image and save it in the specified folder path\n",
    "    download_image(download_link, file_path, title)\n",
    "    \n",
    "    # Store image information in the dictionary\n",
    "    Nasa_Satellite_images[title] = {}\n",
    "    Nasa_Satellite_images[title] = {\n",
    "        \"Image\": download_link,\n",
    "        \"Description\": description\n",
    "    }\n",
    "\n",
    "# Save the image information dictionary as a JSON file\n",
    "with open('Nasa_Satellite.json', 'w') as f:\n",
    "    json.dump(Nasa_Satellite_images, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cc4d3",
   "metadata": {},
   "source": [
    "# IMDB Gender images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af98788",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def download_image(url, folder_path, file_name):\n",
    "    \"\"\"\n",
    "    Download the image from the given URL and save it with the specified file name in the specified folder path.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name + '.jpg')\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# URL and folder path\n",
    "folder_path = r\"C:\\Users\\darvi\\Desktop\\GitHub\\Python-Assignment1\"\n",
    "folder_path = folder_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Folder name\n",
    "folder_name = \"IMDB Gender images\"\n",
    "file_path = os.path.join(folder_path, folder_name)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)\n",
    "    \n",
    "    \n",
    "target = 10001\n",
    "page = list(range(1, target, 50))\n",
    "celebrities = {}\n",
    "categories = [\"female\", \"male\"]\n",
    "\n",
    "for category in categories:\n",
    "    # Loop through the pages\n",
    "    for i in page:\n",
    "        # Get the HTML content of the page\n",
    "        url = f\"https://www.imdb.com/search/name/?gender={category}&start={i}&ref_=rlm\"\n",
    "        html_text = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_text, \"lxml\")\n",
    "\n",
    "        # Extract the information for each celebrity\n",
    "        names = soup.find_all(\"h3\", class_=\"lister-item-header\")\n",
    "        images = soup.find_all(\"img\")\n",
    "        infos = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "        for name, link, info in zip(names, images, infos):\n",
    "            # Get the name and image link\n",
    "            name = name.a.text.strip()\n",
    "            link = link[\"src\"]\n",
    "\n",
    "            # Download the image and save it in the specified folder path\n",
    "            download_image(link, file_path, name)\n",
    "\n",
    "            # Store the image link in the celebrities dictionary\n",
    "            if category not in celebrities:\n",
    "                celebrities[category] = {}\n",
    "            if name not in celebrities[category]:\n",
    "                celebrities[category][name] = {}\n",
    "                celebrities[category][name][\"Image\"] = link\n",
    "\n",
    "            # Get the role and movie information\n",
    "            info_element = info.find(\"p\", class_=\"text-muted text-small\")\n",
    "            info_text = info_element.text.strip() if info_element else \"\"\n",
    "            role, movie = info_text.split(\"|\") if \"|\" in info_text else (info_text, \"\")\n",
    "            celebrities[category][name][\"Role\"] = role.strip()\n",
    "            celebrities[category][name][\"Movie\"] = movie.strip()\n",
    "\n",
    "            # Get the additional history information\n",
    "            info_paragraphs = info.find_all(\"p\")\n",
    "            history_text = info_paragraphs[1].text.strip() if len(info_paragraphs) > 1 else \"\"\n",
    "            celebrities[category][name][\"History\"] = history_text\n",
    "\n",
    "# Save the celebrities dictionary as a JSON file\n",
    "json_file_path = os.path.join(folder_path, \"IMDB_Gender.json\")\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(celebrities, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa51e1",
   "metadata": {},
   "source": [
    "# LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5609185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series analysis ... Data cleaning ... Predictive modeling ... SQL ... Recommender systems ... Random forest ... Data preprocessing ... Ensemble methods ... Data integration ... Data governance ... Recurrent neural networks ... Neural networks ... Model selection ... Reinforcement learning ... Clustering ... Data Science ... Data visualization ... Machine learning ... Cross-validation ... Convolutional neural networks ... Data mining ... Anomaly detection ... Dimensionality reduction ... Text analytics ... Statistical analysis ... Data management ... Feature selection ... Deep learning ... Business intelligence ... Big data ... Computer vision ... Hyperparameter tuning ... Natural language processing ... Feature engineering ... Artificial intelligence ... Regression analysis ... Decision trees ... Data warehousing ... NoSQL ... Bayesian methods ... Database ... Gradient boosting ... Support vector machines ... Unsupervised learning ... "
     ]
    }
   ],
   "source": [
    "# Global parameters\n",
    "country = 'Finland'\n",
    "pages_max = 10  # maximum of pages with job posts extracting per 1 keyword (each page contains max 25 posts)\n",
    "request_and = False  # if True the request will be 'Data AND Science' instead of just 'Data Science'\n",
    "file_name = 'linked-in-jobs_data-science_' + country + '_AND=' + str(request_and) + '.csv'\n",
    "\n",
    "# Keywords for searching requests\n",
    "str_ds = 'Data Science, Big data, Machine learning, Data mining, Artificial intelligence, Predictive modeling, Statistical analysis, Data visualization, Deep learning, Natural language processing, Business intelligence, Data warehousing, Data management, Data cleaning, Feature engineering, Time series analysis, Text analytics, Database, SQL, NoSQL, Neural networks, Regression analysis, Clustering, Dimensionality reduction, Anomaly detection, Recommender systems, Data integration, Data governance'\n",
    "str_ml = 'Machine learning, Data preprocessing, Feature selection, Feature engineering, Data visualization, Model selection, Hyperparameter tuning, Cross-validation, Ensemble methods, Neural networks, Deep learning, Convolutional neural networks, Recurrent neural networks, Natural language processing, Computer vision, Reinforcement learning, Unsupervised learning, Clustering, Dimensionality reduction, Bayesian methods, Time series analysis, Random forest, Gradient boosting, Support vector machines, Decision trees, Regression analysis'\n",
    "DataScience = str_ds.split(', ')\n",
    "MachineLearning = str_ml.split(', ')\n",
    "united_set = set(DataScience) | set(MachineLearning)\n",
    "\n",
    "# Creating a dictionary to store the job postings\n",
    "company_posts = {}\n",
    "\n",
    "def scrape_jobs(keyword, n_max=pages_max * 25):\n",
    "    '''Extract the jobs from LinkedIn using keyword and store the job data in the company_posts dictionary'''\n",
    "    site = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={}&location={}&pageNum=0&start={}'\n",
    "\n",
    "    for i in range(0, n_max, 25):\n",
    "        url = site.format(keyword, 'Finland', str(i))\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers).text\n",
    "\n",
    "        soup = BeautifulSoup(response, 'lxml')\n",
    "        jobs = soup.find_all(\n",
    "            class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "\n",
    "        for job in jobs:\n",
    "            job_title = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "            job_company = job.find('h4', class_='base-search-card__subtitle').text.strip()\n",
    "            job_location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "            job_link = job.find('a', class_='base-card__full-link')['href'] if job.find('a', class_='base-card__full-link') else ' '\n",
    "            job_date = job.find('time', class_=\"job-search-card__listdate\")['datetime'] if job.find('time', class_=\"job-search-card__listdate\") else ' '\n",
    "\n",
    "            # Update the company_posts dictionary\n",
    "            if job_company not in company_posts:\n",
    "                company_posts[job_company] = {keyword: []}\n",
    "            else:\n",
    "                if keyword not in company_posts[job_company]:                   \n",
    "                    company_posts[job_company][keyword] = []\n",
    "\n",
    "            # Append the job details to the corresponding keyword list\n",
    "            company_posts[job_company][keyword].append({\n",
    "                'Title': job_title,\n",
    "                'Location': job_location,\n",
    "                'Link': job_link,\n",
    "                'Date': job_date\n",
    "            })\n",
    "\n",
    "        time.sleep(0.15)\n",
    "\n",
    "\n",
    "# Initialization a cycle with keywords from the united_set\n",
    "separator = '%20AND%20' if request_and else '%20'\n",
    "for item in united_set:\n",
    "    print(item, '...', end=' ')\n",
    "    scrape_jobs(keyword=item.replace(' ', separator))\n",
    "\n",
    "# Writing the data to a CSV file\n",
    "with open(file_name, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Company', 'Keyword', 'Title', 'Location', 'Link', 'Date'])\n",
    "\n",
    "    for company, keyword_data in company_posts.items():\n",
    "        for keyword, posts in keyword_data.items():\n",
    "            for post in posts:\n",
    "                writer.writerow([company, keyword, post['Title'], post['Location'], post['Link'], post['Date']])\n",
    "                \n",
    "# Save the image information dictionary as a JSON file\n",
    "with open('LinkedIn.json', 'w') as f:\n",
    "    json.dump(company_posts, f, indent = 4)              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
